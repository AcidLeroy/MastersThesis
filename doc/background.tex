%\chapter{Background}
\PARstart Human activity classification in videos is not only a difficult problem to solve
algorithmically but also pushes current computing solutions to the edge of their
capability, especially when attempting to keep up with real-time video rates.
Many solutions have been proposed for robust activity classification in videos
\cite{niebles2010modeling} \cite{bashir2007object} \cite{ribeiro2005human}
\cite{karpathy2014large}. Methods such as \cite{bashir2007object} rely on
principal component analysis (PCA) and hidden Markov models (HMMs) to attempt to
classify motions using trajectories; however, all the datasets study consist of
hardware augmentation (a motion tracking device for example), used to track
motions of hands etc, and do not rely solely on the video source for
classification. Our method requires no other augmentation  other than the
videos.  Other methods such as \cite{niebles2010modeling}, use similar
techniques that are done in this paper, however they do not address the
computational burden of producing a histogram of features to use for
classification. We extend parts of their idea and create a distributed  framework that
scales both horizontally and vertically. Finally, new deep learning methods such
as convolutional neural networks are also gaining popularity in activity
classification in videos because there is no need to select the type of features
\textit{a-priori}, you let the network select what is important for
classification \cite{karpathy2014large}. This technique is computationally
expensive, requires huge amounts of data to properly classify activities
\cite{karpathy2014large} and also lacks insight into what is or is not
functioning properly  in the network, essentially rendering the CNN as a black
box. The study presented by Laptev et al. \cite{laptev2008learning} showed that
it is possible to reduce a video dataset size significantly using optical flow
and a technique called bag of features (BoF) which is ultimately used by a
machine learning algorithm, such as non-linear support vector machines to
classify the activities. The BoF technique, however, lacks scalability and
proposes no solution to this problem. Indeed we use a similar method to
reduce the feature set, but we augment the bag of features and propose
solution that is also scalable.

Much of the work that has been done in the area of activity recognition and
video analysis in the cloud is found in
%Table \ref{tab:paper_summary}
the list below.
. Many of these papers present exceptional work in both human activity
classification as well as video analysis using distributed systems. However,
none of them fully address the problem presented here. From
%Table \ref{tab:paper_summary}
the list below
, it is evident that human activity classification as
well as distributed video analysis is an unsolved problem and many researchers
continue to publish in this area. Furthermore, it is obvious that many of these
studies either address activity classification or distributed video systems, but
none effectively combine both ideas.
\begin{itemize}
  \item \textbf{Human activity recognition} \cite{wang2016action} - Uses edge trajectories,
  optical flow histograms, Fisher Vectors but was not implemented to scale
  \item \textbf{Large-scale video analysis} \cite{wang2016cloud}  - Uses  google cloud
  to analyze videos in a secure and robust way but does perform human activity recognition
  \item \textbf{Video segmentation and activity recognition} \cite{kuehne2016end} - Uses Fisher vectors
  , structured temporal models and Guassian  mixed models and also uses SVM and PCA as classification
  technique, but does not prove to be scalable.
  \item \textbf{Action recognition} \cite{cai2016towards} - Uses dynamic trajectory and static deep features
   and Linear SVM and combines deep learning techniques with trajectory features. Again, this solution
   has not proven to be scalable.
   \item \textbf{Cloud resource management} \cite{kaseb2015cloud} - Scaling resources effectively
   on the cloud to minimize cost and maximize performance. This paper addresses how to scale
   properly on the cloud, but doesn't address our human activity recognition needs.
   \item \textbf{Large-scale video classification} \cite{karpathy2014large} - Does not reduce
   feature space for human activity classification
   \item \textbf{Activity classification} \cite{niebles2010modeling}- Uses spatio-temporal synchrony
   , a synchrony auto-encoder and performs well using using local features. This method has
   not been shown to scale.
\end{itemize}


%   \begin{table*}[t]
%   \begin{centering}
%     \begin{tabular}{ | p{0.21\linewidth} | p{0.21\linewidth} | p{0.21\linewidth} | p{0.21\linewidth}| }
%     \hline
%     \textbf{Study} & \textbf{Features} & \textbf{Classification} & \textbf{Comments} \\
%     \hline
%     Human activity recognition \cite{wang2016action}&
%     edge trajectories, optical flow histograms, Fisher Vector & multi-class SVM
%     & Accurate classif. of various
%     human activities using edge trajectories in combination with optical flow \\
%     \hline
%     Large-scale video analysis \cite{wang2016cloud} & N/A & N/A & Using google cloud to analyze videos in a secure and robust way \\
%     \hline
%     Video segmentation and activity recognition \cite{kuehne2016end} & Fisher Vectors, structured
%     temporal models and Gaussian mixed models & SVM and PCA & Classif. of activities as
%     well as attempting to automatically parse out activities in the videos \\
%     \hline
%     Action recognition \cite{cai2016towards} & Dynamic trajectory and static deep features
%     & Linear SVM & Combining deep learning techniques with trajectory features. \\
%     \hline
%     Cloud resource management \cite{kaseb2015cloud} & N/A & N/A & Scaling resources effectively
%     on the cloud to minimize cost and maximize performance.\\
%     \hline
%     Large-scale video classif. \cite{karpathy2014large} & N/A &
%     Convolution Neural Networks & 1 million youtube videos with 487 classes,
%     not implemented on a distributed framework \\
%     \hline
%     Activity classif. \cite{niebles2010modeling} & spatio-temporal synchrony
%     & synchrony auto-encoder & Quick motion estimation using local features
%     and high classif. rate. \\
%     \hline
%     \end{tabular}
%     \caption{Latest work in video analysis in the cloud and human activity classification}
%     \label{tab:paper_summary}
%   \end{centering}
% \end{table*}


% In addition to reviewing the studies in Table \ref{tab:paper_summary},  we also
% summarize many of the datasets used in this field of research in Table
% \ref{tab:common_datasets}. This table illustrates some of the common datasets
% that are used in benchmarking how well a given classification algorithm works in
% comparison with other methods.

The AOLME dataset is much bigger and is
considered to be truly a dataset that is ``in the wild'' than most other ones \cite{soomro2012ucf101},
\cite{niebles2010modeling}, \cite{vo2014stochastic}, \cite{spriggs2009temporal}
\cite{rohrbach2012database}. These
 datasets do not include the learning activities
that we are interested in automatically analyzing and thus are not a good fit
for this research. Though in this paper we use a subset of our entire AOLME
video database, we can in the future leverage large AOLME dataset
as soon as we begin manually classifying and segmenting the videos for
validation of our methods.

%   \begin{table*}[t]
%   \begin{centering}
%     \begin{tabular}{ | p{0.3\linewidth} | p{0.3\linewidth} | p{0.3\linewidth} |  }
%       \hline
%       \textbf{Title} & \textbf{Description} & \textbf{URL} \\
%       \hline
%       UCF101 \cite{soomro2012ucf101}& A dataset of 101 human actions (13,320 videos) &
%       \url{http://www.thumos.info/download.html} \\
%       \hline
%       KTH & Six types of human activity (2391 sequences) & \url{http://www.nada.kth.se/cvap/actions/} \\
%       \hline
%       Olympic \cite{niebles2010modeling} & 16 olympic sports gathered from youtube &
%        \url{http://vision.stanford.edu/Datasets/OlympicSports/} \\
%       \hline
%       Toy Assembly \cite{vo2014stochastic} & 29 sequences of 2-3 minute long sequences
%       of a human assembling a toy from five different bins & \url{http://www.cc.gatech.edu/~nvo9/sin/} \\
%       \hline
%       CMU-MMAC \cite{spriggs2009temporal} & Database that contains multimodal measures of activities such as
%       cooking and food preparation & \url{http://kitchen.cs.cmu.edu/main.php} \\
%       \hline
%       MPIICooking \cite{rohrbach2012database} & Database of 65 cooking activities (8.7GB of AVI formatted video),
%       continuously recorded in a realistic setting & \url{http://tinyurl.com/nvcoh6w} \\
%       \hline
%     \end{tabular}
%     \caption{Common datasets used for activity recognition}
%     \label{tab:common_datasets}
%   \end{centering}
% \end{table*}

\subsection{\label{subsection:activity_classification}Human Activity Classification in Videos}
Human activity classification in video is an unsolved problem and has attracted
a variety of different image and video processing techniques. One of the main
contributing factors to its difficulty is the sheer size of the data, videos
tend to be very large and pose a very interesting problem to the big data
community. Another contributing factor in making videos difficult to classify is
the scale and frequency in which activities appear in the videos. In other
words, if the activity we are searching for is typing, then it is difficult to
find that activity when the action could be taking up the entire frame or only
tens of pixels. Furthermore, we must also account for the speed of the action.
Depending on the frame rate, the actions in the video could appear to be slower
in some scenarios than in others, therefore any algorithm must account for
time-varying activities.

In current big data processing techniques, a method that is employed to reduce
the data size to discover important features about data is known as map-reduce
\cite{dean2008mapreduce}. Many software packages have since been developed to
implement the ideas in \cite{dean2008mapreduce} such as Hadoop, Spark, Storm
and many others. However, these frameworks assume that individual pieces of the
large dataset are atomic and relatively small. For example, Amazon needs to process
information about products that they sell. They can use a relatively small amount
of information to represent this each product (especially compared to videos).
As a result they are able to distribute these small amounts of data to thousands
of clusters to build a product recommender system and associate every product to
every other product to create similarities between products. All of this processing
can easily be done with Hadoop\cite{White:2009:HDG:1717298} or Spark. However,
passing messages around that contain video segments is not feasible with these
systems and is not advised. Since video human activity classification would
require such a message passing system, we innovate in this area by proposing a
framework that is simple, robust and extremely fast using Amazon Web Services
for human activity recognition in videos.

Encoding videos as succinct datasets that can easily be classified and are
representative of the underlying features is another area that this paper
attempts to address. In \textit{Learning to encode motion using spatio-temporal
synchrony} \cite{konda2013learning} they attempt to address this need by detecting
synchrony between frames in videos. They were robustly able to classify motions in
datasets KTH, UCF sports, Hollywood2 and YUPENN in the low to mid 90s classification
rate. However, training and testing the system varied between 2-3 minutes all
the way up to days. In this paper, we not only get very good classification
rates, but because the system is horizontally scalable, we can train the system
in a matter of minutes assuming relatively small video segments (videos on the
order of 1-2 MB).

More modern approaches use various compound techniques in an attempt to solve
human activity recognition such as \cite{wang2016action}. In this paper the
authors showed that they can classify many of the existing datasets very well
using a combination of optical flow histograms, edge trajectories, and support
vector machines for classification. In this paper, we do not attempt to beat
the results in this paper, we strive to provide a method for activity
classification that can distributed on a cloud like infrastructure and that can
be used in real time. Furthermore, the datasets that they use are not from
videos in the wild. Our AOLME dataset contains thousands of hours of video that
are not ideal for training and testing activity recognition algorithms. This
makes tuning and tweaking the parameters for any algorithm, edge trajectories or
not, very difficult. Like them though, we are using optical flow which is well
studied and robust for video activity recognition. An other contribution of this
paper that is not provided in \cite{wang2016action} is that we openly provide
our software as open source so that other studies can be easily conducted.


% \subsection{\label{subsection:cloud_computing}Cloud Computing}
% Cloud computing has become a common practice among scientists, engineers and
% business owners alike for solving computationally expensive problems at a
% relative low cost. Cloud computing offers services on demand rather than
% provisioning hardware and software ahead of time. This has opened the door to
% researchers who require thousands of computers but don't necessarily have the
% funds or access to a cluster of their own \cite{armbrust2009above}. Cloud
% computing offers a model that is pay as you go, you only pay for what you use.
% This contrasts with the traditional way of solving computationally expensive
% problems by acquiring hardware and software ahead of time to run on dedicated
% machines. This has the advantage that the owner of the hardware has full control
% of its resources, but it has the disadvantage of costing a significant amount of
% money and there is a chance that the originally allocated hardware is  either
% insufficient for the task or is overdone,  therefore wasting money and
% computational resources.  At its core, information technology resources are now
% a programmable resource in the cloud, rather than one that has to be manually
% setup and configured.
%
% The programming model for cloud computing is fundamentally different than
% traditional software models. In the traditional infrastructure of computing,
% applications ran on servers and often times relied on the state of a particular
% server to perform computations \cite{awsbestpractices}. That model must be
% broken in order to efficiently develop applications for the cloud. Clearly, not
% all applications can be stateless, but the keys is to eliminate as many stateful
% components as possible and replace them with stateless ones. When this paradigm
% is followed, along with designing software with well defined interfaces, it makes creating a scalable, flexible application easy using cloud
% service providers \cite{awsbestpractices} and how to make auto deployment a snap
% for rapid testing and distribution using container technologies such as docker.
% An example of a well defined interface is a representational state transfer (RESTful) one,
%  which is an interface that provides easy interoperability between compute nodes
%  on a network. Typically an interface like this communicates over http or https.
%
% In the following subsections we explore the current state of cloud computing and
% what types of applications can leverage a cloud like infrastructure, a high
% level overview AWS and a few of the services that can benefit a video processing
% application.

\subsubsection{\label{subsubsection:aws}Amazon Web Services}
Amazon Web Services (AWS) is the most mature cloud solution architecture
available to the public. A popular streaming service in the United States,
Netflix, recently moved all of its streaming services to AWS
\cite{netflixawsmove}. This is important to note because Netflix almost
exclusively serves streaming video to hundreds of thousands of customers
everyday. It's obvious from this move alone that there is a lot of potential in
AWS for processing videos. Netflix cites many reason as to why they moved their
monolithic video streaming app to hundreds of micro services that are run on the
AWS cloud, but a few that truly stand out are \enquote{scalable computing and
storage needs}, \enquote{service availability}, and \enquote{cost reduction}
\cite{netflixawsmove}. All of these reasons are enough for any academic
institution to begin leveraging the power of micro services and cloud
infrastructure.

% AWS provides a number of services that are important to scaling our architecture
% both horizontally and vertically. Several services that are important to
% our video processing paradigm are:
% \begin{itemize}
% \item Elastic compute cloud (EC2) - These are virtual machines that can be
% provisioned quickly and can be configured a number of ways. They are the work
% horses for operating effectively on the AWS cloud. Furthermore, you can
% provision one, a thousand, or have them created as you begin to saturate
% existing instances.
% \item Simple storage service (S3) - This services provides storage that is enormous,
% durable and distributed over regions in the united states. Petabytes of data can
% be easily accessed from anywhere within an Amazon region and have the advantage
% of being redundant and fault tolerant. That is to say, the data is not simply
% stored on a hard drive at amazon, but is replicated over hundreds of machines
% and locations.
% \item Simple queue service (SQS) - Is a distributed queue that allows asynchronous
% reads and writes to any node in a cluster that has access. This service allows
% extremely reliable communication between micro services.
% \item Amazon Auto Scaling (AAS) - Auto scaling is a service that allows users
% to scale their EC2 instances automatically based on CPU and memory saturation. It
% is a key service that allows users to keep costs low yet perform at maximum
% capability.
% \item Elastic container service (ECS) - This services provides orchestration for
% automatic deployment of containers. This is an especially useful service if
% your application is deployed into a Docker container.
% \end{itemize}
%
% The list provided is by no means meant to be exhaustive, but rather provide
% a brief summary of the services that are important to creating an application
% that can be horizontally scaled on the cloud. AWS offers dozens of services, each
% one can be tailored to the specific needs of the developer and the application
% requirements.
%
% EC2 instances are the work horses in the Amazon cloud. Not only are they important
% for horizontal scalability, but they are also important for vertical scalability.
% There are a number of instance types that can be chosen for customized
% applications needs. For example, instances can be provisioned with a single CPU
% of moderate frequency and several hundred MBs of random access memory (RAM) or
% they can be vertically scaled to contain state-of-the-art GPUs with dozens of
% CPU cores and hundreds of GBs of RAM. In fact, AWS is one of the few cloud
% services providers that does provide access to instances with GPUs, which is
% extremely important for video processing algorithms that leverage efficient
% GPU algorithms written in CUDA and/or OpenCL. Thus applications can be scaled
% vertically and horizontally using these EC2 instances.

% \subsubsection{\label{subsubsection:activity_in_the_cloud}Activity Recognition Using the Cloud}
% Until this point, we have only presented the basics of how the cloud can be used
% to perform computationally heavy tasks. In this subsection, we investigate how
% other research has leveraged the cloud to perform similar video processing tasks.
% One of the latest works is ``A Cloud-Based Large-Scale Distributed Video Analysis
% System'' \cite{wang2016cloud}. In this paper, the authors attempt to make a
% robust, scalable and secure platform for performing video analysis using the
% Google cloud computing infrastructure. Their system proposes video analysis
% modules that are pluggable and therefore could perform a multitude of tasks and
% they also detail the data model for processing videos. However, their infrastructure
% lacks the continuous deployment and ease of deployment that the architecture
% in this thesis provides and they do not provide the software they wrote as a
% deliverable as we do. Many of the same concepts are shared between our paper
% and this paper in terms of architecture, but they are not automatically
% orchestrating their services from software development to deployment. Our proposal
% not only supports a cloud like infrastructure for analyzing videos, but also proposes
% a delivery system that is not only convenient for the end users of our video analysis
% package, but is also convenient for developers wishing to plug their particular
% application into our architecture. Furthermore this paper provides all the
% software openly to the public so that experiments can easily be redone and leveraged
% for future work.
