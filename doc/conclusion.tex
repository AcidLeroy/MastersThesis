\chapter{Future Work \& Conclusion}
\section{Future Work}
This thesis investigates the basic idea how to efficiently distribute and classify
human activity in the AOLME dataset and as a result there a few areas where the
research can be greatly improved. The first suggestion to improve the in the field
of this research is to greatly increase the size of truth data for more
statistically significant data. The research done in this paper consists of 40 video
subsets, many of which contained clips from the same footage but at varying times.
Increasing this database to several hundred would improve research here.

We also didn't investigate whether we could accurately classify typing vs writing.
This would be an extension of this research that would be important so that we
could investigate if our algorithm could determine the difference between the two
activities.

In other aspect that would be worth investigating for extending this thesis,
is to attempt to add an interactive aspect to training and testing. In other words,
it would be interesting to precompute and train the machine learning algorithms
on multiple human actions and then interactively searching through the feature
space to then observe the videos that matched the classification. Work in this
area would greatly aid with the manual analysis of the AOLME videos that is
currently being done.

Finally, we found that we didn't get very good results for classifying writing.
Investigation into this would be interesting to see why the results here varied
so much from the results we obtained from typing classification. It's still unknown
why the bag of features failed to classify writing accurately as it did for typing.
As a suspicion, with no supporting evidence, it may have something to do that
general hand movement around paper is similar to that of writing, therefore the
algorithm may struggle for this reason.

\section{Conclusion}
We presented a novel video processing architecture and algorithm for human
activity classification in large video databases such as the AOLME dataset. Our
method is both horizontally and vertically scalable thanks to enabling
technologies in the cloud as well as convenient APIs provided by OpenCV for easy
switching between CPU and GPU implementations of Lucas-Kanade and Farneback
optical flow methods. In addition to the scalability of our system, we also
presented an accurate method for detecting typing in video segments extracted
from the AOLME dataset. Our algorithm greatly reduced the original feature space
of megabytes down to just a few kilobytes, which makes the bandwidth limit on
the cloud very manageable. Because the output feature space is quite small
compared with the original size of the videos input into the system, training
and testing can be done very rapidly and in turn automatic classification can be
done once the system has been trained at near real-time rates.
