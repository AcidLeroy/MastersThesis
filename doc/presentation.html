<!doctype html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
	<link rel="icon" href="http://www.freeiconspng.com/uploads/cloud-icon-17.png">
	<title>Cloud Human Activity Recognition</title>

	<link rel="stylesheet" href="reveal.js/css/reveal.css">
	<link rel="stylesheet" href="reveal.js/css/theme/night.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
	var link = document.createElement( 'link' );
	link.rel = 'stylesheet';
	link.type = 'text/css';
	link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
	document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>
</head>
<body>
	<div class="reveal">
		<div class="slides">
			<section><h2>Distributed and Scalable Video Analysis Architecture for
				Human Activity Recognition Using Cloud Services </h2>
				<hr>
				<br>
				Cody W. Eilar
				<br> <br>University of New Mexico
			</br>
		</section>


		<!--
		*
		* INTRODUCTION
		*
	-->
	<section>
		<section>
			<h2> Introduction </h2>
			<img src='https://media.giphy.com/media/hcwIm2NdTYhva/giphy.gif' height='300'/>
		</section>

		<section>
			<h3>Manual Video Analysis and Segmentation</h3>
			<ul>
				<li>UNM College of Education in collaboration with ivPCL assessing quantifiable metrics for human learning in the classroom</li>
				<li>All videos currently manually annotated</li>
				<li>Videos manually analyzed</li>
				<li>Completely manual process is <b> extremely </b> time consuming
			</ul>
		</section>

		<section data-math=false>
			<h3>Extremely Large Video Datasets</h3>
			<ul>
				<li>Huge AOLME dataset:</li>
				<ul>
					<li>Over 3 years of data collected</li>
					<li>2500 hours of high quality video</li>
					<li>High estimate of cost (dollars) to analyze: 250,000.000 @ 10/hour</li>
					<li>Low estimate of cost (dollars) to analyze: 31,250.00 @ 10/hour</li>
				</ul>
			</ul>
		</section>

		<section>
			<h3>Extremely Large Video Datasets</h3>
			<style type="text/css">
			#videos {
				height: 300;
			}
			</style>
			<table id="videos">
				<tr>
					<th><img src='figures/gifs/typing_seg_18.mp4.gif'/></th>
					<th><img src='figures/gifs/typing_seg_2.mp4.gif'/></th>
					<th><img src='figures/gifs/writing_seg_3.mp4.gif'/></th>
					<th><img src='figures/gifs/typing_seg_4.mp4.gif'/></th>
					<th><img src='figures/gifs/typing_seg_5.mp4.gif'/></th>
				</tr>
				<tr>
					<td ><img src='figures/gifs/typing_seg_6.mp4.gif'/></td>
					<td><img src='figures/gifs/typing_seg_7.mp4.gif'/></td>
					<td ><img src='figures/gifs/writing_seg_8.mp4.gif'/></td>
					<td ><img src='figures/gifs/typing_seg_9.mp4.gif'/></td>
					<td ><img src='figures/gifs/writing_seg_10.mp4.gif'/></td>
				</tr>
				<tr>
					<td><img src='figures/gifs/writing_seg_11.mp4.gif'/></td>
					<td><img src='figures/gifs/writing_seg_12.mp4.gif'/></td>
					<td ><img src='figures/gifs/typing_seg_13.mp4.gif'/></td>
					<td><img src='figures/gifs/typing_seg_14.mp4.gif'/></td>
					<td><img src='figures/gifs/typing_seg_15.mp4.gif'/></td>
				</tr>
				<tr>
					<td><img src='figures/gifs/typing_seg_11.mp4.gif'/></td>
					<td><img src='figures/gifs/typing_seg_12.mp4.gif'/></td>
					<td ><img src='figures/gifs/writing_seg_13.mp4.gif'/></td>
					<td><img src='figures/gifs/typing_seg_14.mp4.gif'/></td>
					<td><img src='figures/gifs/writing_seg_15.mp4.gif'/></td>
				</tr>
			</table>

		</section>

		<section>
			<h3>Vertically and Horizontally Scalable Solutions</h3>
			<ul>
				<li>Vertical: using resources on a single node such as memory, GPUs, CPUs etc.</li>
				<li>Horizontal: use hundreds of compute nodes if necessary</li>
			</ul>
		</section>

		<section>
			<h3>Data Reduction</h3>
			<ul>
				<li>Reducing three-dimensional video cubes of GBs to KBs </li>
				<li>Extremely computationally expensive</li>
				<li>Minimize training time</li>
				<li>Minimize classification time</li>
			</ul>
		</section>

		<section>
			<h3>Open Source Software</h3>
			<ul>
				<li> Facilitation of future research </li>
				<li> Robust reliable software that can be reused and improved </li>
				<li> Enable repeatability of results by anyone </li>
			</ul>
			<img src='http://www.geant.org/News_and_Events/PublishingImages/Pages/Green-light-for-open-source-software-Greenhouse-Special-Interest-Group/greenhouse-sig.jpg'/ height=300>

		</section>

		<section>
			<h3>Affordable Solution for University Research </h3>
			Reduce the cost of doing human activity research by choosing a
			flexible and affordable solution
		</section>

		<section>
			<h3>Thesis Objectives</h3>
			<ul>
				<li>Reduce feature space of videos to:</li>
				<ul>
					<li>Minimize training time</li>
					<li>Minimize classification time</li>
				</ul>
				<li>Design software that can scale horizontally and vertically to maximize efficacy of processing since feature reduction is a computationally expensive operation </li>
				<li>Accurately classify a subset of activities that are of interest to the UNM College of Education </li>
				<ul>
					<li> Typing </li>
					<li> Writing </li>
				</ul>
				<li>Provide core software with Open Source License</li>
			</ul>
		</section>
	</section>


	<!--
	*
	*	BACKGROUND
	*
-->
<section>
	<section>
		<h2> Background</h2>
		<img src='https://media.giphy.com/media/ZKQVJv9u9VIv6/giphy.gif' height='300'/>
	</section>
	<section>
		<h3> Current Methods in Video Feature Reduction for Classification </h3>
		<ul>
			<li> Edge trajectories + optical flow histogram + Fisher Vectors </li>
			<li> Fisher Vectors + structured temporal models + Gaussian mixed Models </li>
			<li> Dynamic trajectory + static deep features</li>
			<li> Spatio-Temporal Synchrony </li>
		</ul>
	</section>

	<section>
		<h3> Techniques for Classifying Reduced Feature Space</h3>
		<ul>
			<li>Linear Support Vector Machines</li>
			<li>Multi Class Support Vector Machines</li>
			<li>Support Vector Machines + Principal Component Analysis</li>
		</ul>
	</section>

	<section>
		<h3> Techniques That Don't Need Reduced Feature Space </h3>
		Convolutional Neural Networks
	</section>

	<section>
		<h3>Video Databases Currently Being Used for Method Validation</h3>

		<table>
			<thead>
				<tr>
					<th>Title</th>
					<th>Description</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><a href='http://www.thumos.info/download.html'>UCF101</a></td>
					<td>A dataset of 101 human actions (13,320 videos)</td>
				</tr>
				<tr>
					<td><a href='http://www.nada.kth.se/cvap/actions/'>KTH</a></td>
					<td>Six types of human activity (2391 sequences)</td>
				</tr>
				<tr>
					<td><a href='http://vision.stanford.edu/Datasets/OlympicSports/'>Olympic</a></td>
					<td>16 olympic sports gathered from youtube</td>
				</tr>
				<tr>
					<td><a href='http://www.cc.gatech.edu/~nvo9/sin/'>Toy Assembly</a></td>
					<td>29 sequences of 2-3 minute long sequences of a human assembling a toy from five different bins</td>
				</tr>

				<tr>
					<td><a href='http://kitchen.cs.cmu.edu/main.php'>CMU-MMAC</a></td>
					<td>Database that contains multimodal measures of activities such
						as cooking and food preparation</td>
					</tr>

					<tr>
						<td><a href='http://tinurl.com/nvcoh6w'>MPIICooking</a></td>
						<td>Database of 65 cooking activities (8.7GB of AVI formatted video),
							continuously recorded in a realistic setting</td>
						</tr>
					</tbody>
				</table>
			</section>

			<section>
				<h3>Accuracy of Provided Methods</h3>
				Most of the methods proved to be highly accurate with provided databases,
				usually within in the range of 70-90%.
				<figure>
					<img src='figures/current_accuracy.png'/>
					<figcaption>Results acquired using edge trajectories with multi-class SVM</figcaption>
				</figure>
			</section>

			<section>
				<h3>Current methods for distributed video analysis </h3>
				<ul>
					<li>Distributed video analysis for toy tasks (i.e. copying and decryption) </li>
					<li>Proprietary software used by Netflix, Amazon, Google and other internet giants
						<li>MapReduce Frameworks for image and text analysis </li>
					</ul>

					<img src='https://pbs.twimg.com/profile_images/1252505253/elephant_rgb_sq.png' height=200/>
					<img src='http://www.hops.io/sites/default/files/styles/portfolio_detail/public/spark_0.png?itok=ClgaSoQj' height=200 style="background-color:#a5a5a5;"/>
					<img src='http://hortonworks.com/wp-content/uploads/2016/03/storm_logo.png' height=200/>
				</section>

				<section>
					<h3> Analysis of Current Methods</h3>
					<ul>
						<li> Human activity recognition methods are great but there are some limitations
							<ul>
								<li>Long training times</li>
								<li>Little proof of scalability</li>
								<li>CNN methods require significant training data</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<h3> Analysis of Current Methods </h3>
					<ul>
						<li>Database limitations
							<ul>
								<li> Many of the videos used do not constitute videos in the wild </li>
								<li> Some video datasets are very limited in the activities that they attempt to classify </li>
								<li> Some of the datasets are relatively small </li>
								<li> Some datasets contain more than just video data, such as trajectory analysis using gyroscopes etc.</li>
							</ul>
						</li>
					</ul>
				</section>

				<section>
					<h3>Analysis of Current Methods</h3>
					<ul>
						<li>Distributed and Scalable Processing Limitations</li>
						<ul>
							<li>Most MapReduce frameworks do not contain out-of-the box functionality for video analysis</li>
							<li>Previously proposed architectures only accomplish trivial tasks, e.g. copying and decryption </li>
						</ul>
					</ul>
				</section>
			</section>

			<!--
			*
			* Methods
			*
		-->
		<section>

			<section>
				<h2> Methods </h2>
				<img src='https://media.giphy.com/media/ROx3xm65zntC0/giphy.gif' height=400/>
			</section>

			<section>
				<h3>Architecture Design</h3>
				<img src='figures/extract_features_dataflow.png' height=600 style="background-color:#a5a5a5;"/>
			</section>

			<section>
				<h3> Life of a Video </h3>
				<img src='figures/life_of_video.png' height=600 style="background-color:#a5a5a5;"/>
			</section>

			<section>
				<h3> Master node configuration </h3>
				Role is to populate video queue with videos from which to extract features
				and then retrieve CDFs from results queue.
			</section>

			<section>
				<h3> Slave node configuration </h3>
				Responsible for popping videos off of video queue, extracting features from
				videos and then pushing results onto the results queue.
			</section>

			<section>
				<h3>Software Deployment</h3>
				<ul>
					<li>Continuous Integration </li>
					<li>Test Driven Development</li>
				</ul>
			</section>

			<section>
				<h3>Continuous Integration </h3>
				<img src='figures/continuous_integration.png' height=600 style="background-color:#a5a5a5;"/>
			</section>


			<section>
				<h3>Vertical Scalability</h3>
				OpenCL in combination with OpenCV's transparent API to enable vertically
				scalable algorithms. I leverage these ideas heavily for optical flow calculations.
			</section>

			<section>
				<h3> Optical Flow Implementation </h3>
				Developed using OpenCV functions but with generic interface that leverages
				high performance dependency injection. Easily swap out algorithms, TAPI, and
				video readers
				<img src='figures/dependency_injection.png' style="background-color:#a5a5a5;"/>
			</section>

			<section>
				<h3> Comparison of Methods </h3>
				<ul>
					<li> Though Lucas-Kanade was faster at calculating motion estimation,
						machine learning proved to not classify them well
					</li>
					<li> Farneback method, or dense optical flow, proved to provide better features
						for machine learning to shatter the class with a hyperplane </li>
					</ul>
					<img src='http://docs.opencv.org/trunk/opticalflow_lk.jpg' height=300/>
					<img src='http://docs.opencv.org/trunk/opticalfb.jpg' height=300/>
				</section>

				<section>
					<h3> Feature Extraction From Optical Flow </h3>
					<ul>
						<li> Large video datasets are broken into small 1-5MB videos to be processed by each node </li>
						<li> Videos are then broken into a handful of CDFs </img>
						</ul>
						<img src='figures/extract_features_flow.png' style="background-color:#a5a5a5;"/>
					</section>


					<section>
						<h3>Classifying the Reduced Feature Space </h3>
						<ul>
							<li>Linear Support Vector Machines</li>
							<li>K Nearest Neighbors </li>
							<li>Train and testing on 40 typing/no typing and 40 writing/no writing video segments
							</ul>
						</section>

						<section>
							<h3> Challenges </h3>
							<ul>
								<li>Matching C++ real-time code to prototype MATLAB code </li>
								<li>GPU pass through in a Docker container </li>
								<li>Results looked good for a small subset of writing data (around 8 videos),
									but proved to be inadequate as more data points were added </li>
									<li>Making compute cluster easy to deploy required serious software engineering </li>
								</ul>

							</section>

						</section>

						<section>
							<section>
								<h2> Results & Discussion </h2>
								<img src='https://media.giphy.com/media/3oEjI6hkw6nbYNQkz6/giphy.gif'/>
							</section>

							<section>
								<h3> Classification Results </h3>
								<table>
									<tr>
										<th> </th>
										<th>Typing</th>
										<th>No Typing</th>
									</tr>
									<tr>
										<td> <b>Typing</b> </td>
										<td> 19 </td>
										<td>1 </td>
									</tr>
									<tr>
										<td> <b>No Typing</b> </td>
										<td> 3 </td>
										<td>17 </td>
									</tr>
								</table> <br>
								Accuracy: 90%<br>
								<img src='figures/typing.gif' width=300/>
								<img src='figures/gifs/no_typing.gif' width=300/>
							</section>

							<section>
								<h3> Classification Results </h3>
								<table>
									<tr>
										<th> </th>
										<th>Writing</th>
										<th>No Writing</th>
									</tr>
									<tr>
										<td> <b>Writing</b> </td>
										<td> 18 </td>
										<td>2 </td>
									</tr>
									<tr>
										<td> <b>No Writing</b> </td>
										<td> 11 </td>
										<td>9 </td>
									</tr>
								</table> <br>
								Accuracy: 65%<br>
								<img src='figures/writing.gif' width=300/>
								<img src='figures/gifs/no_writing.gif' width=300/>
							</section>

							<section>
								<h3>Proof of Scalability </h3>
								<img src='figures/speed_up.png'/>
							</section>

							<section>
								<h3>High Performance Local Node vs Cloud </h3>
								<img src='figures/aws_vs_local.png'/>
							</section>

							<section>
								<h3> S3 Performance for Downloading and Uploading Video Segments </h3>
								<img src='figures/s3_download_speed.png' width=400/>
								<img src='figures/s3_upload_speed.png' width=400
							</section>
						</section>

						<section>
							<h3> Future Work </h3>
							<ul>
								<li>Investigate low accuracy with writing </li>
								<li>Create larger AOLME ground truth dataset for more generalizable results </li>
								<li>Investigate real-time interaction with extracting and classifying features </li>
								<li>Expand work to autoscale based on size of video list queue</li>
							</ul>
						</section>

						<section>
							<h3>Conclusion</h3>
							<ul>
								<li>In this thesis I have accomplished: </li>
								<ul>
									<li> Horizontally and vertically scalable video analysis architecture </li>
									<li> A novel feature extraction algorithm that greatly reduces feature space </li>
									<li> A free and open source software package that is continuous deployed and tested </li>
									<li> An novel and accurate algorithm for detecting typing in videos </li>
								</ul>
							</ul>
						</section>
					</section>

					<section>
						<h2> Questions? </h2>
					</section>


				</div>
			</div>

			<script src="reveal.js/lib/js/head.min.js"></script>
			<script src="reveal.js/js/reveal.js"></script>

			<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'reveal.js/plugin/markdown/marked.js' },
					{ src: 'reveal.js/plugin/markdown/markdown.js' },
					{ src: 'reveal.js/plugin/math/math.js', async: true },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
			</script>
		</body>
		</html>
